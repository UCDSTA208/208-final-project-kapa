% !TEX root = main.tex

\subsection{Purpose}

It is important to study what people did and create our study on top of it. In this section, we focus on summarize previous studies of spam/ham e-mail filtering and their machine learning methods. At the end of this section, we point out the new methods and new data we use in this project to show our understanding.  \\

\subsection{What We Learn}
People already came up with the idea of spam/ham e-mail filtering before 2004. In the paper \textit{Machine Learning Techniques in Spam Filtering} written by Konstantin in 2004\cite{MLinspam}, the experiment used four main methods : Naive Bayes, K-NN, Perceptron, and SVM. And compare accuracy rate of each methods. In this basic practice, they found the Perceptron method has the highest accuracy rate, 98.5\% with a corpus of 1099 messages. \\

\begin{figure}[H]
	\centering
	\includegraphics[scale=1.0, width=\linewidth]{./plots/2004.png}
	\caption{Summary of \textit{Machine Learning Techniques in Spam Filtering\cite{MLinspam}}}
	\label{paper_summary_2004}
\end{figure}

In 2010, \textit{Email Spam Filtering using Supervised Machine Learning Techniques } written by V.Christina\cite{EmailMLT}, they used Naive Bayes, J-48(Decision Tree) and Multilayer Perceptron. And they found out MLP performed the best with 99.3\% accuracy rate when experimenting with a corpus of 1500 messages.\\

\begin{figure}[H]
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=.4\linewidth]{./plots/2010-1.png}
		\includegraphics[width=.4\linewidth]{./plots/2010-2.png}
	\end{subfigure}%
	\caption{Summary of \textit{Email Spam Filtering using Supervised Machine Learning Techniques\cite{EmailMLT}}}
	\label{paper_summary_2010}
\end{figure}



In 2016, \textit{Spam Mail Detection using Classification} written by Parhat and Gambhir used Naive Bayes, SVM and J-48(Decision Tree)\cite{MailDectionCL}. And they found out Naive Bayes performed the best with 76\% accuracy rate in their experiment. \\ 
And \textit{Email Spam Detection} written by Ge and  Lauren\cite{EmailDection}, used the corpus from TREC 2007 with 1000 messages. They tried logistic regression, Naive Bayesian, Decision Tree and K-NN. The finally found KNN with highest 99\% accuracy rate. \\

\begin{figure}[H]
	\centering
	\includegraphics[width=10cm]{./plots/2016.png}
	\caption{Performance of Different Classifiers\cite{EmailDection}}
	\label{paper_summary_2016}
\end{figure}


The summary of methods in each previous studies by year is shown in Table \ref{method_summary}. 

\begin{table}[H]
	\centering
	\caption{Methods Summary}
	\label{method_summary}
	\begin{tabular}{lcccccc||c}
		\hline
		Year & NB & KNN   & SVM  & Decision Tree & MLP & Logistic & Best Model  \\ \hline
		2004\cite{MLinspam} &  V &  V    & V     &  \quad  & V Perceptron & \quad& MLP(98.5\%) \\
		2010\cite{EmailMLT} &  V & \quad & \quad & V C4.5  & V            & \quad& MLP(99.3\%) \\ 
		2016\cite{MailDectionCL} &  V &    V  & V     & V  J48  &   V & V      & KNN(99\%)  \\ \hline
	\end{tabular}
\end{table}




%\begin{figure}[H]
%	\centering
%	\includegraphics[width=10cm]{Method_Summary.png}
%	\caption{Methods Summary}
%	\label{method_summary}
%\end{figure}

\subsection{Our Works}

\begin{enumerate}
	\item \textbf{Use multiple data source}: In each paper, they mainly use a single year of corpus data. In our project, we tried to source different e-mail and integrate them. The format of each data source is different thus hard to clean. And we successfully got to manage a huge data set. 

	\item \textbf{Try 6 methods at the same time}: Previous studies compare accuracy rate with different methods, but they didn't compare them all at a time. So we studied the methods from 2004 to 2016, and apply all of possible methods with adequate tuning parameters to compare them 

	\item \textbf{Apply Uni-Gram and Bi-Gram}: Each paper marked that data processing step is important to a good result. Here, we introduced bag of words of Uni-Gram and Bi-Gram methods in the feature engineering part. And we can see different result of accuracy rate in the following section.
\end{enumerate}
